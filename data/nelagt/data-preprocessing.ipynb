{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48034233",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING\n",
    "\n",
    "In this notebook we will pre-process the NELA-GT-22 dataset. It will allow us to filter the data by time and by newspaper. Then, we will compute similarities between items (pieces of news) and from that transition probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea498b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python dependencies\n",
    "import os\n",
    "import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# import utils dependencies\n",
    "from preprocessing_utils import execute_query_pandas, zero_diagonal, \\\n",
    "                                cut_links, row_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63d9d2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/python_venv/arw/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# create instance of Transformer to compute the embedding of each item\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02bf969",
   "metadata": {},
   "source": [
    "#### Case Study\n",
    "- News from the same newspaper (**The Guardian**) for a three month period (January to April)\n",
    "- News from the same newspaper (**The New York Times**) for a two week perdiod (2022-01-01 to 2022-01-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62a837d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 620/620 [00:12<00:00, 51.09it/s]\n"
     ]
    }
   ],
   "source": [
    "path = 'nela-gt-2022.db'\n",
    "sources = ['theguardian']\n",
    "#sources = ['abcnews']\n",
    "# Note that we need to add extra quotes around each source's name\n",
    "# for the query to work properly e.g.: \"'thenewyorktimes'\"\n",
    "sources_str = [\"'%s'\" % s for s in sources]\n",
    "query = \"SELECT * FROM newsdata WHERE source IN (%s)\" % \",\".join(sources_str)\n",
    "\n",
    "df = execute_query_pandas(path, query)\n",
    "# convert date to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# map item id for convenience\n",
    "map_dict_items = {old_id:new_id for new_id,old_id in enumerate(df.id.unique())}\n",
    "df.replace({'id':map_dict_items}, inplace=True)\n",
    "\n",
    "\n",
    "# compute a similarity_matrix between all the items within each time slot\n",
    "time_slots = [('2022-01-01','2022-04-01')]\n",
    "for time_slot in time_slots:\n",
    "    # temporal df of the time slot\n",
    "    tmp_df = df[(df.date>=time_slot[0])&(df.date<time_slot[1])]\n",
    "    # drop duplicate items\n",
    "    tmp_df = tmp_df.drop_duplicates(subset='content')\n",
    "    # save\n",
    "    tmp_df.to_csv(f'pandas_df/df_{sources[0]}_from_{time_slot[0]}_to_{time_slot[1]}')\n",
    "    \n",
    "    # compute embeddings\n",
    "    embeddings = list()\n",
    "    for sentence in tqdm.tqdm(tmp_df.content):\n",
    "        embeddings.append(model.encode(sentence))\n",
    "    \n",
    "    # compute similarities between pieces of news\n",
    "    similarities = cosine_similarity(np.array(embeddings))\n",
    "    \n",
    "    # zero diagonal\n",
    "    similarities = zero_diagonal(similarities)\n",
    "    \n",
    "    # save similarity_matrix between all items (not normalized)\n",
    "    with open(f'similarity_matrices/similarity_matrix_{sources[0]}_from_{time_slot[0]}_to_{time_slot[1]}', 'wb') as f:\n",
    "        pickle.dump(similarities,f)\n",
    "\n",
    "    # cut links to get the top_k\n",
    "    for k in [5,10,20]:\n",
    "        similarities_k = cut_links(similarities,k).astype(np.float64)\n",
    "\n",
    "        # normalize the rows of the matrix so that they represent transition probabilities\n",
    "        trans_prob = row_normalize(similarities_k)\n",
    "        # save transition_probabilities\n",
    "        with open(f'transition_probabilities/transition_probabilities_{sources[0]}_from_{time_slot[0]}_to_{time_slot[1]}_k_{k}', 'wb') as f:\n",
    "            pickle.dump(trans_prob,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reachability",
   "language": "python",
   "name": "reachability"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
